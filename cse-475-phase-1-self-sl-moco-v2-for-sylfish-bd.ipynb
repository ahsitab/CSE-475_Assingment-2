{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Environment Setup & Dependency Resolution**\n",
    "\n",
    "\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "The default Kaggle environment comes with pre-installed libraries (like `numpy` 2.0+) that are currently incompatible with `ultralytics` and `lightly`. Standard `!pip install` commands often fail because system paths prioritize the pre-installed (broken) versions.\n",
    "\n",
    "\n",
    "\n",
    "### **The Solution: Isolated Environment**\n",
    "\n",
    "To bypass these conflicts, this cell implements a robust \"Target Directory\" fix:\n",
    "\n",
    "1.  **Create Custom Directory:** We create a private folder at `/kaggle/working/my_env`.\n",
    "\n",
    "2.  **Force Installation:** We install stable, compatible versions of `numpy` (1.x), `scipy` (1.13.1), and our project tools (`ultralytics`, `lightly`) directly into this folder.\n",
    "\n",
    "3.  **Path Injection:** We insert this folder at the **top** of Python's system path (`sys.path`), forcing the notebook to load our working libraries instead of the system defaults.\n",
    "\n",
    "\n",
    "\n",
    "**Expected Outcome:**\n",
    "\n",
    "* Numpy version should be `1.26.x`.\n",
    "\n",
    "* SciPy version should be `1.13.1`.\n",
    "\n",
    "* This ensures `MoCo V2` training runs without \"AttributeError\" crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "# 1. Create a folder for our custom libraries\n",
    "\n",
    "# We install here so Kaggle's system libraries can't mess with us\n",
    "\n",
    "TARGET_DIR = \"/kaggle/working/my_env\"\n",
    "\n",
    "if not os.path.exists(TARGET_DIR):\n",
    "    os.makedirs(TARGET_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Add this folder to the VERY TOP of the system path\n",
    "\n",
    "# This forces Python to load our working libraries before the broken system ones\n",
    "\n",
    "if TARGET_DIR not in sys.path:\n",
    "    sys.path.insert(0, TARGET_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Force Install Stable Versions into that folder\n",
    "\n",
    "# We exclude 'torch' to save download time (system torch usually works with numpy 1.x)\n",
    "\n",
    "print(\"ðŸš€ Installing stable libraries into private environment... (This takes ~2 mins)\")\n",
    "\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\",\n",
    "    \"--target\", TARGET_DIR,  # Install into our folder\n",
    "    \"numpy<2.0\",             # Force Numpy 1.x\n",
    "    \"scipy==1.13.1\",         # Force Stable SciPy\n",
    "    \"ultralytics\",\n",
    "    \"lightly\",\n",
    "    \"pytorch-lightning\",\n",
    "    \"--upgrade\",\n",
    "    \"--no-user\"              # Ignore user's home directory\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâœ… Installation Complete. Verifying versions...\")\n",
    "\n",
    "import numpy\n",
    "\n",
    "import scipy\n",
    "\n",
    "print(f\"ðŸ”¹ Numpy Version: {numpy.__version__} (Should be 1.26.x)\")\n",
    "\n",
    "print(f\"ðŸ”¹ SciPy Version: {scipy.__version__} (Should be 1.13.1)\")\n",
    "\n",
    "# Remove the problematic logging library entirely\n",
    "\n",
    "!pip uninstall -y tensorboard\n",
    "\n",
    "# **CSE 475: Phase 1 - Self-Supervised Learning (MoCo V2) for SylFish_BD**\n",
    "\n",
    "\n",
    "\n",
    "## **1. Objective**\n",
    "\n",
    "This notebook implements the **MoCo V2 (Momentum Contrast)** pipeline.\n",
    "\n",
    "* **Goal:** Pre-train the **YOLOv12n Backbone** on unlabeled data.\n",
    "\n",
    "* **Input:** 80% Unlabeled images from the SylFish_BD dataset.\n",
    "\n",
    "* **Output:** Learned weights (`yolo_moco_v2_backbone.pth`) that understand visual features.\n",
    "\n",
    "\n",
    "\n",
    "## **2. Dataset**\n",
    "\n",
    "We use the SylFish_BD dataset (9 fish species: boal, ilish, kalibaush, katla, koi, mrigel, pabda, rui, telapia). We will only use the **images** and ignore the labels for this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. RECONNECT TO CUSTOM ENVIRONMENT ---\n",
    "\n",
    "# We must run this block every time we restart the kernel\n",
    "\n",
    "TARGET_DIR = \"/kaggle/working/my_env\"\n",
    "\n",
    "if TARGET_DIR not in sys.path:\n",
    "    sys.path.insert(0, TARGET_DIR)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"âœ… Connected to custom environment at: {TARGET_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. NOW IMPORT LIBRARIES ---\n",
    "\n",
    "# Since Python now knows where to look, these imports will work\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import lightly\n",
    "import ultralytics\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. LOAD DATA FROM KAGGLE INPUT ---\n",
    "\n",
    "# SylFish_BD dataset is available at /kaggle/input/syfish-bd/Sylfish_bd\n",
    "\n",
    "input_dir = Path('/kaggle/input/syfish-bd/Sylfish_bd')\n",
    "images_dir = input_dir / 'images'\n",
    "\n",
    "\n",
    "\n",
    "# Create unlabeled directory (80% of images)\n",
    "\n",
    "UNLABELED_DIR = '/kaggle/working/ssl_experiment/unlabeled_data/images'\n",
    "\n",
    "os.makedirs(UNLABELED_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Collect all images from all fish species\n",
    "\n",
    "classes = ['boal', 'ilish', 'kalibaush', 'katla', 'koi', 'mrigel', 'pabda', 'rui', 'telapia']\n",
    "\n",
    "all_images = []\n",
    "\n",
    "\n",
    "\n",
    "for cls in classes:\n",
    "    cls_dir = images_dir / cls\n",
    "    if cls_dir.exists():\n",
    "        images = list(cls_dir.glob('*.jpg'))\n",
    "        all_images.extend(images)\n",
    "        print(f\"   Found {len(images)} images in {cls}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Total images collected: {len(all_images)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Copy 80% of images to unlabeled directory (for MoCo V2)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(all_images)\n",
    "unlabeled_count = int(len(all_images) * 0.8)\n",
    "unlabeled_images = all_images[:unlabeled_count]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š Using {len(unlabeled_images)} images (80%) for MoCo V2 pre-training...\")\n",
    "\n",
    "\n",
    "\n",
    "# Copy images to unlabeled directory\n",
    "\n",
    "for img_path in unlabeled_images:\n",
    "    shutil.copy(img_path, UNLABELED_DIR)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"âœ… Data Ready at: {UNLABELED_DIR}\")\n",
    "print(f\"âœ… Total unlabeled images: {len(os.listdir(UNLABELED_DIR))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. YOLO-MoCo V2 Model Architecture**\n",
    "\n",
    "\n",
    "\n",
    "This cell defines the custom PyTorch Lightning module **`YoloMoCoV2`**, which adapts the YOLOv12 architecture for momentum contrast learning. It performs the following key operations:\n",
    "\n",
    "\n",
    "\n",
    "### **3.1 Warning Suppression**\n",
    "\n",
    "* **Purpose:** The code first suppresses specific `UserWarnings` related to `pydantic`.\n",
    "\n",
    "* **Reason:** The `lightly` and `ultralytics` libraries use metadata attributes (`repr`, `frozen`) in a way that generates harmless but noisy warnings in the current Kaggle environment. Silencing them keeps the training logs clean.\n",
    "\n",
    "\n",
    "\n",
    "### **3.2 Model Surgery (Backbone Extraction)**\n",
    "\n",
    "* **Loading:** We initialize a standard `yolov12n.pt` model.\n",
    "\n",
    "* **Extraction:** We surgically extract the **first 10 layers** (indices 0-9) of the model. This corresponds to the **CSP-Darknet Backbone**, which is responsible for feature extraction (edges, textures, shapes). The detection head is discarded.\n",
    "\n",
    "\n",
    "\n",
    "### **3.3 The \"Shape Fix\" (Adaptive Pooling)**\n",
    "\n",
    "* **The Problem:** YOLO backbones output multi-scale feature maps (e.g., `256 x 20 x 20`). Passing this directly to a linear layer often causes shape mismatch errors (e.g., `mat1 and mat2 shapes cannot be multiplied`).\n",
    "\n",
    "* **The Solution:** We insert an `nn.AdaptiveAvgPool2d((1, 1))` layer.\n",
    "\n",
    "* **Effect:** This forces the spatial dimensions to `1x1` regardless of the input size, flattening the features into a clean vector (e.g., `[Batch, 256]`). This guarantees compatibility with the projection head.\n",
    "\n",
    "\n",
    "\n",
    "### **3.4 MoCo V2 Components**\n",
    "\n",
    "* **Projection Head:** A multi-layer perceptron (MLP) that maps the backbone features into a 128-dimensional latent space.\n",
    "\n",
    "* **Loss Function:** We use **NT-Xent Loss** (Normalized Temperature-scaled Cross Entropy), which forces the model to pull positive pairs (two views of the same image) closer together while pushing negative pairs apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "# Suppress specific Pydantic warnings to keep the output clean\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'repr' attribute with value False\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'frozen' attribute with value True\")\n",
    "\n",
    "print(\"âœ… Pydantic warnings silenced.\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YoloMoCoV2(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name='yolo12n.pt'):\n",
    "        super().__init__()\n",
    "        # 1. Load YOLO\n",
    "        yolo = YOLO(model_name)\n",
    "        \n",
    "        # 2. Extract Backbone (Layers 0-9)\n",
    "        self.backbone = nn.Sequential(*list(yolo.model.model.children())[:10])\n",
    "        \n",
    "        # 3. Add Global Average Pooling (THE FIX)\n",
    "        # This crushes spatial dimensions (H, W) -> (1, 1)\n",
    "        # Guarantees the output vector size is just 'Channels' (e.g., 512 or 256)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 4. Check actual channel count\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 640, 640)\n",
    "            out = self.backbone(dummy)\n",
    "            if isinstance(out, list): out = out[-1]\n",
    "            \n",
    "            # Now we pool it to see the channel count\n",
    "            pooled = self.avg_pool(out)\n",
    "            in_features = pooled.flatten(1).shape[1]\n",
    "            print(f\"âœ… Detected Backbone Channels: {in_features}\")\n",
    "            \n",
    "        # 5. Projection Head\n",
    "        self.projection_head = SimCLRProjectionHead(in_features, 512, 128)\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone Forward\n",
    "        features = self.backbone(x)\n",
    "        if isinstance(features, list): features = features[-1]\n",
    "        \n",
    "        # Pool & Flatten\n",
    "        pooled = self.avg_pool(features)\n",
    "        h = pooled.flatten(start_dim=1)\n",
    "        \n",
    "        # Project\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=0.08, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… Fixed YOLO-MoCo V2 Model Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. MoCo V2 Training Execution**\n",
    "\n",
    "\n",
    "\n",
    "This cell executes the self-supervised pre-training loop. It consists of three key stages:\n",
    "\n",
    "\n",
    "\n",
    "### **4.1 Logging & Environment Setup**\n",
    "\n",
    "* **Warning Suppression:** We filter out specific `UserWarnings` and `BrokenPipe` errors common in distributed training on Jupyter notebooks. This ensures the output remains readable without affecting training stability.\n",
    "\n",
    "\n",
    "\n",
    "### **4.2 Data Augmentation & Loading**\n",
    "\n",
    "* **MoCo V2 Transform:** We rely on `lightly.transforms.SimCLRTransform` to automatically generate two augmented views for every image. Augmentations include random cropping, color jittering, and Gaussian blur.\n",
    "\n",
    "* **DataLoader:** Configured for **2x T4 GPUs** with a batch size of 1024 (effective batch size = 2048).\n",
    "\n",
    "\n",
    "\n",
    "### **4.3 Model Training**\n",
    "\n",
    "We initialize the `YoloMoCoV2` model and train it using the `PyTorch Lightning` Trainer with specific optimizations:\n",
    "\n",
    "* **`strategy=\"ddp_notebook\"`:** Enables Distributed Data Parallel training compatible with interactive kernels.\n",
    "\n",
    "* **`sync_batchnorm=True`:** Synchronizes Batch Normalization statistics across both GPUs. This is **critical for Contrastive Learning**, as it prevents the model from using local batch statistics to \"cheat\" the contrastive task.\n",
    "\n",
    "* **`precision=\"16-mixed\"`:** Uses Mixed Precision training to reduce VRAM usage and speed up computations on T4 GPUs.\n",
    "\n",
    "\n",
    "\n",
    "**Output:** The script saves the learned backbone weights to `yolo_moco_v2_backbone.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# 1. Suppress PyTorch Lightning & Distributed Warnings\n",
    "\n",
    "os.environ[\"TORCH_CPP_LOG_LEVEL\"] = \"ERROR\"\n",
    "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"OFF\"\n",
    "\n",
    "\n",
    "\n",
    "# 2. Filter Python Warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Specific filters for the Pydantic/Tensorboard noise\n",
    "warnings.filterwarnings(\"ignore\", message=\".*'repr' attribute.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*'frozen' attribute.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*support for historical reasons.*\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Mute Library Loggers\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch.distributed\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… Warnings suppressed.\")\n",
    "\n",
    "from lightly.transforms.simclr_transform import SimCLRTransform\n",
    "from lightly.data import LightlyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# 1. Setup Transforms\n",
    "# 128x128 is safe for this massive batch size. \n",
    "# If you crash with OOM, reduce batch_size to 512.\n",
    "transform = SimCLRTransform(input_size=128) \n",
    "\n",
    "\n",
    "\n",
    "# 2. Load Dataset\n",
    "# Ensure UNLABELED_DIR is defined (e.g. from your data download cell)\n",
    "if 'UNLABELED_DIR' not in locals():\n",
    "    # Fallback if variable is lost\n",
    "    print(\"âš ï¸ Finding dataset path...\")\n",
    "    import glob\n",
    "    UNLABELED_DIR = '/kaggle/working/ssl_experiment/unlabeled_data/images'\n",
    "    if not os.path.exists(UNLABELED_DIR):\n",
    "        possible_dirs = glob.glob(\"/kaggle/working/**/unlabeled_data/images\", recursive=True)\n",
    "        UNLABELED_DIR = possible_dirs[0] if possible_dirs else \"ERROR_PATH_NOT_FOUND\"\n",
    "\n",
    "\n",
    "\n",
    "dataset = LightlyDataset(input_dir=UNLABELED_DIR, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Create DataLoader (Optimized)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=2048,         \n",
    "    shuffle=True, \n",
    "    num_workers=2,           # Max 2 workers per GPU on Kaggle\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"âœ… Data Loader Ready. Effective Batch Size: 2048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# 1. Setup Logger\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "logger = CSVLogger(save_dir=working_dir, name=\"moco_v2_logs\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"ðŸš€ Starting MoCo V2 Training (Batch 2048, 30 Epochs)...\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Initialize\n",
    "\n",
    "model = YoloMoCoV2(model_name='yolo12n.pt')\n",
    "\n",
    "\n",
    "\n",
    "# 3. Train\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,           \n",
    "    accelerator=\"gpu\", \n",
    "    devices=2,               \n",
    "    strategy=\"ddp_notebook\", \n",
    "    sync_batchnorm=True,     \n",
    "    precision=\"16-mixed\",    # Critical for large batches to save memory\n",
    "    log_every_n_steps=1,     \n",
    "    logger=logger,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, dataloader)\n",
    "    \n",
    "    # 4. Save Weights\n",
    "    save_path = os.path.join(working_dir, 'yolo_moco_v2_backbone.pth')\n",
    "    torch.save(model.backbone.state_dict(), save_path)\n",
    "    print(f\"âœ… Weights Saved: {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training Failed: {e}\")\n",
    "    print(\"   (If OOM occurs, reduce batch_size in Cell 2 to 512)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Appendix: Visual Verification (Sanity Check)**\n",
    "\n",
    "\n",
    "\n",
    "### **Objective**\n",
    "\n",
    "Since Self-Supervised Learning (SSL) does not use labels, we cannot evaluate it using standard metrics like Accuracy or mAP during the pre-training phase. Instead, we perform a **Qualitative Evaluation** using **Nearest Neighbor Retrieval**.\n",
    "\n",
    "\n",
    "\n",
    "**The Hypothesis:** If the MoCo V2 backbone has successfully learned to \"see\" visual features (shapes, textures, orientations), then images that look similar to humans should have mathematically similar feature vectors (embeddings).\n",
    "\n",
    "\n",
    "\n",
    "### **Methodology**\n",
    "\n",
    "1.  **Feature Extraction:** We pass a random subset of 200 images through our frozen, pre-trained YOLOv12 backbone. We use **Global Average Pooling** to squash the output into a single 1D vector (embedding) for each image.\n",
    "\n",
    "2.  **Similarity Search:** We use the **k-Nearest Neighbors (k-NN)** algorithm with **Cosine Similarity** to find images in the dataset that are closest to a given query image in the embedding space.\n",
    "\n",
    "3.  **Visualization:** We display the **Query Image** alongside its top 3 **Matches**.\n",
    "\n",
    "\n",
    "\n",
    "### **How to Interpret Results**\n",
    "\n",
    "* **âœ… Success:** The matches share semantic characteristics with the query (e.g., similar fish species match together, or similar lighting conditions match). This proves the model has learned a structured representation space.\n",
    "\n",
    "* **âŒ Failure:** The matches appear completely random. This typically indicates that the model needs more training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "WEIGHTS_PATH = 'yolo_moco_v2_backbone.pth'\n",
    "# Double check this path exists!\n",
    "IMAGE_DIR = '/kaggle/working/ssl_experiment/unlabeled_data/images'\n",
    "NUM_SAMPLES = 200\n",
    "NUM_QUERIES = 4\n",
    "\n",
    "\n",
    "\n",
    "# 1. Verify Data Exists First\n",
    "\n",
    "print(f\"ðŸ“‚ Checking image directory: {IMAGE_DIR}\")\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    print(f\"âŒ Error: Directory does not exist. Did you run the 'prepare_data' step?\")\n",
    "    # Try to find where the images actually are\n",
    "    print(\"ðŸ” Searching for images in current directory...\")\n",
    "    found_imgs = glob.glob(\"**/*.jpg\", recursive=True)\n",
    "    if found_imgs:\n",
    "        print(f\"   Found images at: {os.path.dirname(found_imgs[0])}\")\n",
    "        IMAGE_DIR = os.path.dirname(found_imgs[0])\n",
    "        print(f\"   -> Updated IMAGE_DIR to: {IMAGE_DIR}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No images found! You need to re-run the Data Download/Split step.\")\n",
    "\n",
    "\n",
    "\n",
    "image_files = glob.glob(os.path.join(IMAGE_DIR, '*'))\n",
    "print(f\"âœ… Found {len(image_files)} images.\")\n",
    "\n",
    "\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    raise ValueError(\"Image directory is empty!\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load Backbone\n",
    "\n",
    "print(\"ðŸ”„ Loading Backbone...\")\n",
    "base_model = YOLO('yolo12n.pt')\n",
    "backbone = torch.nn.Sequential(*list(base_model.model.model.children())[:10])\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(WEIGHTS_PATH):\n",
    "    state_dict = torch.load(WEIGHTS_PATH)\n",
    "    # Strict=False to ignore any potential head mismatches\n",
    "    backbone.load_state_dict(state_dict, strict=False)\n",
    "    print(\"âœ… Weights Loaded!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Weights not found. Using random weights.\")\n",
    "\n",
    "\n",
    "\n",
    "backbone.eval()\n",
    "backbone.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "# 3. Extract Features (Debug Mode)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "if len(image_files) > NUM_SAMPLES:\n",
    "    image_files = random.sample(image_files, NUM_SAMPLES)\n",
    "\n",
    "\n",
    "\n",
    "features = []\n",
    "valid_files = []\n",
    "\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š Extracting features from {len(image_files)} images...\")\n",
    "\n",
    "\n",
    "\n",
    "for i, img_path in enumerate(image_files):\n",
    "    # No try-except block -> We want to see the error!\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    tensor = preprocess(img).unsqueeze(0).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feat = backbone(tensor)\n",
    "        if isinstance(feat, list): feat = feat[-1]\n",
    "\n",
    "        # Adaptive pool to ensure 1x1 output regardless of input size\n",
    "        feat = torch.nn.functional.adaptive_avg_pool2d(feat, (1, 1))\n",
    "        feat = feat.flatten().cpu().numpy()\n",
    "\n",
    "    features.append(feat)\n",
    "    valid_files.append(img_path)\n",
    "\n",
    "    if i % 50 == 0: print(f\"   Processed {i}...\")\n",
    "\n",
    "\n",
    "\n",
    "features = np.array(features)\n",
    "print(f\"âœ… Extracted features shape: {features.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# 4. Nearest Neighbors\n",
    "\n",
    "if features.shape[0] < 5:\n",
    "    print(\"âŒ Not enough features to run neighbors search.\")\n",
    "else:\n",
    "    print(\"ðŸ” Finding matches...\")\n",
    "    nbrs = NearestNeighbors(n_neighbors=4, metric='cosine').fit(features)\n",
    "    query_indices = random.sample(range(len(features)), NUM_QUERIES)\n",
    "\n",
    "    fig, axs = plt.subplots(NUM_QUERIES, 4, figsize=(15, 4 * NUM_QUERIES))\n",
    "    for i, query_idx in enumerate(query_indices):\n",
    "        distances, indices = nbrs.kneighbors([features[query_idx]])\n",
    "\n",
    "        # Plot Query\n",
    "        axs[i, 0].imshow(Image.open(valid_files[query_idx]))\n",
    "        axs[i, 0].set_title(\"Query\", color='blue')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Plot Matches\n",
    "        for j in range(1, 4):\n",
    "            idx = indices[0][j]\n",
    "            dist = distances[0][j]\n",
    "            axs[i, j].imshow(Image.open(valid_files[idx]))\n",
    "            axs[i, j].set_title(f\"Match {j} ({1-dist:.2f})\")\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
